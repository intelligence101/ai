{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist \n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy \n",
    "from keras.datasets import mnist \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense \n",
    "from keras.layers import Dropout \n",
    "from keras.utils import np_utils "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Flatten \n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras import backend as K \n",
    "K.set_image_dim_ordering('th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = 7\n",
    "numpy.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dataViz(X_train):\n",
    "    \n",
    "    plt.subplot(221)\n",
    "    plt.imshow(X_train[0], cmap=plt.get_cmap('gray'))\n",
    "    plt.subplot(222)\n",
    "    plt.imshow(X_train[1], cmap=plt.get_cmap('gray'))\n",
    "    plt.subplot(223)\n",
    "    plt.imshow(X_train[2], cmap=plt.get_cmap('gray'))\n",
    "    plt.subplot(224)\n",
    "    plt.imshow(X_train[3], cmap=plt.get_cmap('gray'))\n",
    "    #show the plot\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAT8AAAD8CAYAAAABraMFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n5kdETmLzIyIn/R+KxB3xQTzcVwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xde7e978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataViz(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 2 1 0 4 1 4 9 5 9]\n"
     ]
    }
   ],
   "source": [
    "print(y_test[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## flatten data and normalize data \n",
    "def preprocess_for_baseline():\n",
    "    num_pixels = X_train.shape[1]*X_train.shape[2]\n",
    "    X_train_b = X_train.reshape(X_train.shape[0], num_pixels).astype('float32')\n",
    "    X_test_b = X_test.reshape(X_test.shape[0], num_pixels).astype('float32')\n",
    "    y_train_b = np_utils.to_categorical(y_train)\n",
    "    y_test_b = np_utils.to_categorical(y_test)\n",
    "    num_classes = y_test_b.shape[1]\n",
    "    return X_train_b, y_train_b, X_test_b, y_test_b, num_classes, num_pixels\n",
    "\n",
    "X_train_b, y_train_b, X_test_b, y_test_b, num_classes, num_pixels = preprocess_for_baseline()\n",
    "\n",
    "X_train_b = X_train_b / 255 \n",
    "X_test_b = X_test_b / 255 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(y_test_b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "print(y_test_b[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def baseline_model():\n",
    "    #create model \n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_pixels, input_dim=num_pixels, kernel_initializer ='normal', activation='relu'))\n",
    "    model.add(Dense(num_classes, kernel_initializer='normal', activation = 'softmax'))\n",
    "    #compile model \n",
    "    model.compile(loss='categorical_crossentropy', optimizer = 'adam', metrics =['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "2s - loss: 0.2806 - acc: 0.9209 - val_loss: 0.1423 - val_acc: 0.9568\n",
      "Epoch 2/10\n",
      "1s - loss: 0.1123 - acc: 0.9677 - val_loss: 0.0940 - val_acc: 0.9711\n",
      "Epoch 3/10\n",
      "1s - loss: 0.0727 - acc: 0.9791 - val_loss: 0.0812 - val_acc: 0.9756\n",
      "Epoch 4/10\n",
      "1s - loss: 0.0505 - acc: 0.9855 - val_loss: 0.0761 - val_acc: 0.9770\n",
      "Epoch 5/10\n",
      "1s - loss: 0.0372 - acc: 0.9895 - val_loss: 0.0663 - val_acc: 0.9793\n",
      "Epoch 6/10\n",
      "1s - loss: 0.0268 - acc: 0.9929 - val_loss: 0.0665 - val_acc: 0.9797\n",
      "Epoch 7/10\n",
      "1s - loss: 0.0202 - acc: 0.9951 - val_loss: 0.0642 - val_acc: 0.9795\n",
      "Epoch 8/10\n",
      "1s - loss: 0.0137 - acc: 0.9971 - val_loss: 0.0611 - val_acc: 0.9821\n",
      "Epoch 9/10\n",
      "1s - loss: 0.0108 - acc: 0.9977 - val_loss: 0.0584 - val_acc: 0.9813\n",
      "Epoch 10/10\n",
      "1s - loss: 0.0084 - acc: 0.9984 - val_loss: 0.0589 - val_acc: 0.9821\n",
      "Baseline Error: 1.79%\n"
     ]
    }
   ],
   "source": [
    "def run_baseline():\n",
    "    model = baseline_model() \n",
    "    # Fit the model \n",
    "    model.fit(X_train_b, y_train_b, validation_data=(X_test_b,y_test_b), epochs=10, batch_size=200, verbose=2)\n",
    "    scores = model.evaluate(X_test_b,y_test_b, verbose =0)\n",
    "    print(\"Baseline Error: %.2f%%\"%(100-scores[1]*100))\n",
    "    \n",
    "run_baseline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Convnet for mnist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_for_conv1():\n",
    "    \n",
    "    X_train_c1 = X_train.reshape(X_train.shape[0], 1, 28, 28).astype('float32')\n",
    "    X_test_c1 = X_test.reshape(X_test.shape[0], 1, 28, 28).astype('float32')\n",
    "    X_train_c1 = X_train_c1 / 255 \n",
    "    X_test_c1 = X_test_c1 /255 \n",
    "    y_train_c1 = np_utils.to_categorical(y_train)\n",
    "    y_test_c1 = np_utils.to_categorical(y_test)\n",
    "    num_classes = y_test_c1.shape[1]\n",
    "    return X_train_c1, y_train_c1, X_test_c1, y_test_c1, num_classes \n",
    "\n",
    "X_train_c1, y_train_c1, X_test_c1, y_test_c1, num_classes = preprocess_for_conv1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_convnet():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32,(5,5),input_shape=(1,28,28),activation = 'relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128,activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    ## compile model \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "11s - loss: 0.2441 - acc: 0.9301 - val_loss: 0.0727 - val_acc: 0.9783\n",
      "Epoch 2/10\n",
      "11s - loss: 0.0696 - acc: 0.9798 - val_loss: 0.0471 - val_acc: 0.9853\n",
      "Epoch 3/10\n",
      "11s - loss: 0.0513 - acc: 0.9848 - val_loss: 0.0394 - val_acc: 0.9877\n",
      "Epoch 4/10\n",
      "11s - loss: 0.0384 - acc: 0.9882 - val_loss: 0.0369 - val_acc: 0.9890\n",
      "Epoch 5/10\n",
      "11s - loss: 0.0311 - acc: 0.9905 - val_loss: 0.0425 - val_acc: 0.9866\n",
      "Epoch 6/10\n",
      "11s - loss: 0.0268 - acc: 0.9916 - val_loss: 0.0385 - val_acc: 0.9873\n",
      "Epoch 7/10\n",
      "11s - loss: 0.0225 - acc: 0.9926 - val_loss: 0.0307 - val_acc: 0.9896\n",
      "Epoch 8/10\n",
      "11s - loss: 0.0177 - acc: 0.9942 - val_loss: 0.0331 - val_acc: 0.9898\n",
      "Epoch 9/10\n",
      "11s - loss: 0.0166 - acc: 0.9948 - val_loss: 0.0357 - val_acc: 0.9890\n",
      "Epoch 10/10\n",
      "11s - loss: 0.0129 - acc: 0.9961 - val_loss: 0.0319 - val_acc: 0.9896\n",
      "Baseline Error: 1.04%\n"
     ]
    }
   ],
   "source": [
    "def run_simple_convnet():\n",
    "    model = simple_convnet() \n",
    "    # Fit the model \n",
    "    model.fit(X_train_c1, y_train_c1, validation_data=(X_test_c1,y_test_c1), epochs=10, batch_size=200, verbose=2)\n",
    "    scores = model.evaluate(X_test_c1,y_test_c1, verbose =0)\n",
    "    print(\"Baseline Error: %.2f%%\"%(100-scores[1]*100))\n",
    "    \n",
    "run_simple_convnet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now lets define a larger and more sophisticated model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def larger_conv():\n",
    "    model = Sequential() \n",
    "    model.add(Conv2D(30,(5,5), input_shape=(1,28,28), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Conv2D(15,(3,3),activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128,activation='relu'))\n",
    "    model.add(Dense(50,activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer = 'adam', metrics =['accuracy'])\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "12s - loss: 0.3640 - acc: 0.8898 - val_loss: 0.0706 - val_acc: 0.9780\n",
      "Epoch 2/50\n",
      "11s - loss: 0.0967 - acc: 0.9704 - val_loss: 0.0491 - val_acc: 0.9837\n",
      "Epoch 3/50\n",
      "11s - loss: 0.0689 - acc: 0.9784 - val_loss: 0.0400 - val_acc: 0.9866\n",
      "Epoch 4/50\n",
      "11s - loss: 0.0556 - acc: 0.9833 - val_loss: 0.0358 - val_acc: 0.9870\n",
      "Epoch 5/50\n",
      "11s - loss: 0.0479 - acc: 0.9855 - val_loss: 0.0307 - val_acc: 0.9898\n",
      "Epoch 6/50\n",
      "11s - loss: 0.0430 - acc: 0.9861 - val_loss: 0.0349 - val_acc: 0.9883\n",
      "Epoch 7/50\n",
      "11s - loss: 0.0361 - acc: 0.9890 - val_loss: 0.0315 - val_acc: 0.9899\n",
      "Epoch 8/50\n",
      "11s - loss: 0.0323 - acc: 0.9896 - val_loss: 0.0265 - val_acc: 0.9908\n",
      "Epoch 9/50\n",
      "11s - loss: 0.0307 - acc: 0.9901 - val_loss: 0.0240 - val_acc: 0.9912\n",
      "Epoch 10/50\n",
      "11s - loss: 0.0294 - acc: 0.9907 - val_loss: 0.0251 - val_acc: 0.9918\n",
      "Epoch 11/50\n",
      "11s - loss: 0.0268 - acc: 0.9913 - val_loss: 0.0230 - val_acc: 0.9923\n",
      "Epoch 12/50\n",
      "11s - loss: 0.0260 - acc: 0.9914 - val_loss: 0.0248 - val_acc: 0.9923\n",
      "Epoch 13/50\n",
      "11s - loss: 0.0233 - acc: 0.9923 - val_loss: 0.0265 - val_acc: 0.9924\n",
      "Epoch 14/50\n",
      "11s - loss: 0.0204 - acc: 0.9934 - val_loss: 0.0240 - val_acc: 0.9917\n",
      "Epoch 15/50\n",
      "11s - loss: 0.0194 - acc: 0.9937 - val_loss: 0.0242 - val_acc: 0.9925\n",
      "Epoch 16/50\n",
      "11s - loss: 0.0185 - acc: 0.9940 - val_loss: 0.0227 - val_acc: 0.9924\n",
      "Epoch 17/50\n",
      "11s - loss: 0.0172 - acc: 0.9943 - val_loss: 0.0249 - val_acc: 0.9918\n",
      "Epoch 18/50\n",
      "11s - loss: 0.0162 - acc: 0.9947 - val_loss: 0.0227 - val_acc: 0.9929\n",
      "Epoch 19/50\n",
      "11s - loss: 0.0161 - acc: 0.9946 - val_loss: 0.0239 - val_acc: 0.9926\n",
      "Epoch 20/50\n",
      "11s - loss: 0.0164 - acc: 0.9944 - val_loss: 0.0218 - val_acc: 0.9940\n",
      "Epoch 21/50\n",
      "11s - loss: 0.0160 - acc: 0.9945 - val_loss: 0.0235 - val_acc: 0.9932\n",
      "Epoch 22/50\n",
      "11s - loss: 0.0136 - acc: 0.9953 - val_loss: 0.0239 - val_acc: 0.9931\n",
      "Epoch 23/50\n",
      "11s - loss: 0.0146 - acc: 0.9953 - val_loss: 0.0229 - val_acc: 0.9932\n",
      "Epoch 24/50\n",
      "11s - loss: 0.0129 - acc: 0.9955 - val_loss: 0.0199 - val_acc: 0.9934\n",
      "Epoch 25/50\n",
      "11s - loss: 0.0120 - acc: 0.9961 - val_loss: 0.0232 - val_acc: 0.9923\n",
      "Epoch 26/50\n",
      "11s - loss: 0.0115 - acc: 0.9960 - val_loss: 0.0301 - val_acc: 0.9922\n",
      "Epoch 27/50\n",
      "11s - loss: 0.0129 - acc: 0.9955 - val_loss: 0.0291 - val_acc: 0.9917\n",
      "Epoch 28/50\n",
      "11s - loss: 0.0115 - acc: 0.9962 - val_loss: 0.0266 - val_acc: 0.9928\n",
      "Epoch 29/50\n",
      "11s - loss: 0.0118 - acc: 0.9958 - val_loss: 0.0296 - val_acc: 0.9914\n",
      "Epoch 30/50\n",
      "11s - loss: 0.0102 - acc: 0.9964 - val_loss: 0.0231 - val_acc: 0.9937\n",
      "Epoch 31/50\n",
      "11s - loss: 0.0105 - acc: 0.9964 - val_loss: 0.0278 - val_acc: 0.9930\n",
      "Epoch 32/50\n",
      "11s - loss: 0.0089 - acc: 0.9969 - val_loss: 0.0278 - val_acc: 0.9921\n",
      "Epoch 33/50\n",
      "11s - loss: 0.0096 - acc: 0.9966 - val_loss: 0.0237 - val_acc: 0.9933\n",
      "Epoch 34/50\n",
      "11s - loss: 0.0097 - acc: 0.9966 - val_loss: 0.0257 - val_acc: 0.9940\n",
      "Epoch 35/50\n",
      "11s - loss: 0.0101 - acc: 0.9965 - val_loss: 0.0274 - val_acc: 0.9933\n",
      "Epoch 36/50\n",
      "11s - loss: 0.0096 - acc: 0.9969 - val_loss: 0.0230 - val_acc: 0.9936\n",
      "Epoch 37/50\n",
      "11s - loss: 0.0099 - acc: 0.9965 - val_loss: 0.0249 - val_acc: 0.9929\n",
      "Epoch 38/50\n",
      "11s - loss: 0.0087 - acc: 0.9975 - val_loss: 0.0240 - val_acc: 0.9927\n",
      "Epoch 39/50\n",
      "11s - loss: 0.0087 - acc: 0.9972 - val_loss: 0.0273 - val_acc: 0.9921\n",
      "Epoch 40/50\n",
      "11s - loss: 0.0083 - acc: 0.9972 - val_loss: 0.0252 - val_acc: 0.9929\n",
      "Epoch 41/50\n",
      "11s - loss: 0.0077 - acc: 0.9973 - val_loss: 0.0235 - val_acc: 0.9935\n",
      "Epoch 42/50\n",
      "11s - loss: 0.0073 - acc: 0.9976 - val_loss: 0.0271 - val_acc: 0.9929\n",
      "Epoch 43/50\n",
      "11s - loss: 0.0079 - acc: 0.9972 - val_loss: 0.0268 - val_acc: 0.9918\n",
      "Epoch 44/50\n",
      "11s - loss: 0.0083 - acc: 0.9971 - val_loss: 0.0224 - val_acc: 0.9937\n",
      "Epoch 45/50\n",
      "11s - loss: 0.0078 - acc: 0.9972 - val_loss: 0.0253 - val_acc: 0.9933\n",
      "Epoch 46/50\n",
      "11s - loss: 0.0071 - acc: 0.9971 - val_loss: 0.0267 - val_acc: 0.9930\n",
      "Epoch 47/50\n",
      "11s - loss: 0.0066 - acc: 0.9978 - val_loss: 0.0283 - val_acc: 0.9930\n",
      "Epoch 48/50\n",
      "11s - loss: 0.0068 - acc: 0.9978 - val_loss: 0.0276 - val_acc: 0.9936\n",
      "Epoch 49/50\n",
      "11s - loss: 0.0059 - acc: 0.9980 - val_loss: 0.0285 - val_acc: 0.9933\n",
      "Epoch 50/50\n",
      "11s - loss: 0.0071 - acc: 0.9975 - val_loss: 0.0254 - val_acc: 0.9928\n",
      "Baseline Error: 0.72%\n"
     ]
    }
   ],
   "source": [
    "def run_larger_convnet():\n",
    "    model = larger_conv() \n",
    "    # Fit the model \n",
    "    model.fit(X_train_c1, y_train_c1, validation_data=(X_test_c1,y_test_c1), epochs=50, batch_size=200, verbose=2)\n",
    "    scores = model.evaluate(X_test_c1,y_test_c1, verbose =0)\n",
    "    print(\"Baseline Error: %.2f%%\"%(100-scores[1]*100))\n",
    "    \n",
    "run_larger_convnet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
